# -*- coding: utf-8 -*-
"""Proyek Kedua TimeSeries_Muhamad Adin Palimbani

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Us1r9tIosrFRJ4P65dGletmCP5_Zy6Tf
"""

import numpy as np
import tensorflow as tf
import yfinance as yf
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
from pandas_datareader.data import DataReader

import pandas as pd
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional
from keras.datasets import imdb

from google.colab import drive
drive.mount ('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/pengembangan-ML-dicoding/kalimati_tarkari_dataset.csv',
                 parse_dates=["Date"],
                 index_col=["Date"]) # parse the date column (tell pandas column 1 is a datetime))
df.head()

len(df)

# How many data do we have?
print('There are', df.shape[0], 'data in this dataset')

# Do we have duplicates?
print('Number of Duplicates:', len(df[df.duplicated()]))

# Do we have missing values?
missing_values = df.isnull().sum()
print('Number of Missing Values by column:\n',missing_values)

print('Number of Missing Values:', df.isnull().sum().sum())

df.replace("", np.nan, inplace=True)
missing_values = df.isnull().sum()
print('Number of Missing Values by column after replacement:\n',missing_values)
print('Number of Missing Values after replacement:', df.isnull().sum().sum())

# Commodity is string value
df['Commodity'] = df['Commodity'].astype(str)

#find 10 comm0dities
top_commodities = df['Commodity'].value_counts().nlargest(10).index.tolist()
top_commodities

df = df[df.Commodity == 'Ginger']
df.drop(['Commodity'],1,inplace=True)

df

len(df) #ginger only

# get the first and last date from the index
first_date = df.index.min().strftime('%Y-%m-%d')
last_date = df.index.max().strftime('%Y-%m-%d')

# combine the dates into a single string
date_range = f"{first_date} to {last_date}"
date_range

plt.figure(figsize=(10, 6))
plt.plot(df.index, df['Average'])
plt.title(f"values from from {date_range}", fontsize=16)
plt.xlabel('Datetime')
plt.ylabel('Average')
plt.show()

data = df.filter(['Average'])
dataset = data.values
data_train_len = int(np.ceil(len(dataset)* 0.8))

print(data_train_len)

data_train = dataset[:data_train_len]
#data_train = data_train.flatten()
data_test = dataset[data_train_len:]
#data_test = data_test.flatten()


print(data_train.shape)
print(data_test.shape)

skala_data = MinMaxScaler(feature_range=(0,1))
data_train_normalisasi = skala_data.fit_transform(data_train)
data_train_normalisasi = data_train_normalisasi.flatten()

data_test_normalisasi = skala_data.fit_transform(data_test)
data_test_normalisasi = data_test_normalisasi.flatten()

dataset_normalisasi = skala_data.fit_transform(dataset)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer): #code ini mengubah series menjadi numpy yang dapat diterima oleh model
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(data_train_normalisasi, window_size=60, batch_size=100, shuffle_buffer=1000)
validation_set = windowed_dataset(data_test_normalisasi, window_size=60, batch_size=100, shuffle_buffer=1000)
# Reset states generated by Keras
tf.keras.backend.clear_session()

# Build the model
model = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),
  tf.keras.layers.Dense(256, activation='relu'),
  tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 100.0)
])

minMae = (dataset_normalisasi.max() - dataset_normalisasi.min()) * 10/100
minMae

# Set the training parameters
class myEarlyStop(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<minMae and logs.get('val_mae')<minMae):
      self.model.stop_training = True

def scheduler(epoch, lr):
  if epoch < 10:
    return lr
  else:
    return lr * tf.math.exp(-0.1)

callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(train_set, validation_data = (validation_set), epochs=100, callbacks=[callback,myEarlyStop()])

mae = history.history['mae']
val_mae = history.history['val_mae']
loss = history.history['loss']
val_loss = history.history['val_loss']

plt.plot(mae, 'b', label='mae')
plt.plot(val_mae, 'g', label='val_mae')

plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.show()

plt.plot(loss, 'b', label='loss')
plt.plot(val_loss, 'g', label='val_loss')

plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()